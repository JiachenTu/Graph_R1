# Taming the Agent: A Systematic Study on Divergence Control and Model Scaling for RL-Incentivized Graph Reasoning Models

## Abstract

Recent advances in agentic GraphRAG frameworks, particularly those optimized via end-to-end reinforcement learning (RL) like Group Relative Policy Optimization (GRPO), have shown remarkable success in complex graph reasoning tasks. However, while prior work demonstrates that such agents *can* achieve strong performance, the optimization process remains a "black box"---practitioners lack principled guidance on how hyperparameters shape training dynamics. In particular, the KL divergence penalty is typically treated as a fixed implementation detail, yet its role in governing the crucial **exploration-exploitation trade-off** for graph reasoning is poorly understood. In this work, we conduct the first systematic investigation into the optimization dynamics of GRPO for agentic graph reasoning on the `2WikiMultiHopQA` benchmark, analyzing two critical axes: KL divergence control and base model scale. Our findings reveal three key insights: (1) the KL coefficient is not merely a stability mechanism but the **primary lever controlling exploration-exploitation balance**, with both insufficient and excessive regularization causing substantial degradation; (2) we uncover the **"overthinking paradox"**---weaker policies generate more reasoning turns while achieving worse performance, demonstrating that reasoning quality dominates quantity; and (3) larger base models provide **multiplicative advantages**, exhibiting controlled exploration and "capability breakthroughs" that smaller models cannot replicate. Based on these findings, we provide actionable guidelines to demystify RL-based graph reasoning and build more robust agentic systems.

## Introduction

Large Language Models (LLMs), despite their remarkable capabilities, often suffer from factual inaccuracies and "hallucinations". Retrieval-Augmented Generation (RAG) was introduced as a foundational solution, grounding model outputs in external knowledge. However, standard RAG methods, which rely on unstructured text chunks, struggle to capture complex, multi-hop relationships. This limitation has spurred the development of GraphRAG frameworks, which leverage knowledge graphs to enhance retrieval and reasoning.

The new frontier in this domain is the integration of end-to-end Reinforcement Learning (RL) with agentic graph reasoning. Foundational work, such as Graph-R1, has demonstrated the power of this approach. By modeling retrieval as a multi-turn agentic process and optimizing it with algorithms like Group Relative Policy Optimization (GRPO), these systems have achieved state-of-the-art performance on complex, multi-hop reasoning benchmarks. This agentic, RL-driven paradigm is rapidly becoming the new standard for high-fidelity graph reasoning.

However, while these works prove that RL agents *can* be effectively trained for graph reasoning, the *process* of achieving this remains a "black box." The stability and final performance of these models are critically dependent on a delicate balance of algorithmic components, yet their optimization dynamics are poorly understood. Specifically, the KL divergence penalty---a mechanism central to modern RL-for-LLM optimization---is typically treated as a fixed implementation detail. Yet this coefficient is not merely a regularizer; it governs the fundamental *exploration-exploitation trade-off* that is uniquely critical in graph reasoning: insufficient regularization may lead to policy drift and ungrounded outputs, while excessive regularization can prevent the agent from discovering effective multi-step reasoning paths. This knowledge gap is a significant barrier to reproducible and robust model development.

This challenge is further compounded by the impact of base model capacity. Prior work has shown that larger models achieve better performance, but deeper questions remain unanswered: How does model scale interact with RL optimization dynamics? Do larger models learn more efficiently, or do they exhibit fundamentally different training behaviors? Understanding this interaction is essential for practitioners who must balance computational cost against performance gains.

In this work, we conduct the first systematic investigation to demystify the optimization dynamics of GRPO for agentic graph reasoning. Using the Graph-R1 framework as our experimental testbed, we focus on the challenging `2WikiMultiHopQA` benchmark, which demands precise multi-step reasoning. Our study is designed around two primary axes: (1) a comprehensive analysis of varying KL divergence coefficients to quantify their impact on training stability and reasoning accuracy, and (2) a comparative study of model scaling within the Qwen series.

Our findings reveal three key insights. First, we establish that the KL coefficient is not merely a stability mechanism but the **primary lever controlling the exploration-exploitation trade-off**---performance exhibits striking non-monotonic sensitivity, with both insufficient (β=0) and excessive (β=0.1) regularization causing substantial degradation, and optimal performance at β=0.01, a tenfold increase from the default used in prior work. Second, we uncover the **"overthinking paradox"**: weaker policies generate more reasoning turns while achieving worse performance, demonstrating that reasoning quality dominates quantity and challenging the naive assumption that more retrieval leads to better answers. This finding connects to broader observations about overthinking in agentic systems. Third, we demonstrate that larger base models provide **multiplicative rather than additive advantages**, exhibiting controlled exploration and "capability breakthroughs" where multi-hop reasoning emerges rapidly---phenomena that smaller models cannot replicate regardless of training duration.

Based on our extensive empirical results, we provide actionable guidelines for the community. Our contributions are:

1. We present the first systematic analysis of GRPO optimization dynamics for agentic graph reasoning, moving beyond prior proofs-of-concept to a rigorous, reproducible investigation.
2. We establish that the KL divergence coefficient is the primary control for the exploration-exploitation trade-off, identifying an optimal operating point that substantially outperforms default configurations.
3. We uncover the "overthinking paradox," revealing that reasoning efficiency---not reasoning quantity---determines performance, with weaker policies compensating through excessive but ineffective retrieval.
4. We characterize the multiplicative scaling advantages of larger base models, including distinct optimization regimes and emergent capability breakthroughs that fundamentally alter the training landscape.

## Acknowledgement

This repo builds upon [Graph-R1](https://github.com/LHRLAB/Graph-R1) [[paper](https://arxiv.org/abs/2507.21892)]. Thanks for their wonderful work.
